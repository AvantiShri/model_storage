{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the saved Keras 1.2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm tf_order_record_5_model_PQzyq_modelWeights.h5\n",
    "!rm tf_order_record_5_model_PQzyq_modelJson.json\n",
    "!wget https://raw.githubusercontent.com/AvantiShri/model_storage/4143cfce7e61611d4c42984578e420cd7556c4b6/deeplift/genomics/tf_order_record_5_model_PQzyq_modelWeights.h5\n",
    "!wget https://raw.githubusercontent.com/AvantiShri/model_storage/4143cfce7e61611d4c42984578e420cd7556c4b6/deeplift/genomics/tf_order_record_5_model_PQzyq_modelJson.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "f = h5py.File(\"tf_order_record_5_model_PQzyq_modelWeights.h5\")\n",
    "print(\"keras version\", f.attrs['keras_version'])\n",
    "print(\"keras json\", json.dumps(json.loads(open(\"tf_order_record_5_model_PQzyq_modelJson.json\").read()),\n",
    "                               indent=4),\"\\n\")\n",
    "print(\"layer weight names:\", [layer_name+\"/\"+x\n",
    "                              for layer_name in f['model_weights'].keys()\n",
    "                              for x in f['model_weights'][layer_name].attrs['weight_names']],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "print (\"keras version\",keras.__version__)\n",
    "import numpy as np\n",
    "\n",
    "#create a keras 2 model with the same architecture\n",
    "#set the weights for each layer using the hdf5\n",
    "#weights file\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv1D(filters=50, kernel_size=11,\n",
    "                              strides=1,\n",
    "                              input_shape=(200,4)))\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.Conv1D(filters=50, kernel_size=11,\n",
    "                              strides=1))\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(units=50))\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(units=3))\n",
    "model.add(keras.layers.Activation(\"sigmoid\"))\n",
    "model.build()\n",
    "\n",
    "#load the weights into each layer\n",
    "\n",
    "#first convolution\n",
    "model.layers[0].set_weights(\n",
    "    [(np.array(f['model_weights']['convolution2d_1/convolution2d_1_W'])\n",
    "              .squeeze().transpose((1,0,2))),\n",
    "     np.array(f['model_weights']['convolution2d_1/convolution2d_1_b'])])\n",
    "#second convolution\n",
    "model.layers[2].set_weights(\n",
    "    [np.array(f['model_weights']['convolution2d_2/convolution2d_2_W']).squeeze(),\n",
    "     np.array(f['model_weights']['convolution2d_2/convolution2d_2_b'])])\n",
    "#first dense layer\n",
    "model.layers[5].set_weights(\n",
    "    [np.array(f['model_weights']['dense_1/dense_1_W']),\n",
    "     np.array(f['model_weights']['dense_1/dense_1_b'])])\n",
    "#second dense layer\n",
    "model.layers[8].set_weights(\n",
    "    [np.array(f['model_weights']['dense_2/dense_2_W']),\n",
    "     np.array(f['model_weights']['dense_2/dense_2_b'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm sequences.simdata.gz\n",
    "!wget https://raw.githubusercontent.com/AvantiShri/model_storage/db919b12f750e5844402153233249bb3d24e9e9a/deeplift/genomics/sequences.simdata.gz\n",
    "!rm test.txt.gz\n",
    "!wget https://raw.githubusercontent.com/AvantiShri/model_storage/9aadb769735c60eb90f7d3d896632ac749a1bdd2/deeplift/genomics/test.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import simdna\n",
    "except ImportError:\n",
    "    print(\"installing simdna package\")\n",
    "    !pip install -e \"git://github.com/kundajelab/simdna.git@0.4.0#egg=simdna\"\n",
    "    print(\"\\n**********************************************************\")\n",
    "    print(\"RESTART THE JUPYTER KERNEL TO PICK UP ON THE INSTALLATION!!!\")\n",
    "    print(\"************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import simdna.synthetic as synthetic\n",
    "reload(synthetic)\n",
    "reload(synthetic.core)\n",
    "import gzip\n",
    "data_filename = \"sequences.simdata.gz\"\n",
    "#read in the data in the testing set\n",
    "test_ids_fh = gzip.open(\"test.txt.gz\",\"rb\")\n",
    "ids_to_load = [x.rstrip(\"\\n\") for x in test_ids_fh]\n",
    "data = synthetic.read_simdata_file(data_filename, ids_to_load=ids_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#this model was trained on data one-hot encoded as a 2d image,\n",
    "#with the row-axis being the axis for one-hot encoding.\n",
    "def one_hot_encode_along_row_axis(sequence):\n",
    "    #theano dim ordering, uses row axis for one-hot\n",
    "    to_return = np.zeros((len(sequence),4), dtype=np.int8)\n",
    "    seq_to_one_hot_fill_in_array(zeros_array=to_return,\n",
    "                                 sequence=sequence,\n",
    "                                 one_hot_axis=1)\n",
    "    return to_return\n",
    "\n",
    "def seq_to_one_hot_fill_in_array(zeros_array, sequence, one_hot_axis):\n",
    "    assert one_hot_axis==0 or one_hot_axis==1\n",
    "    if (one_hot_axis==0):\n",
    "        assert zeros_array.shape[1] == len(sequence)\n",
    "    elif (one_hot_axis==1):\n",
    "        assert zeros_array.shape[0] == len(sequence)\n",
    "    #zeros_array should be an array of dim 4xlen(sequence), filled with zeros.\n",
    "    #will mutate zeros_array\n",
    "    for (i,char) in enumerate(sequence):\n",
    "        if (char==\"A\" or char==\"a\"):\n",
    "            char_idx = 0\n",
    "        elif (char==\"C\" or char==\"c\"):\n",
    "            char_idx = 1\n",
    "        elif (char==\"G\" or char==\"g\"):\n",
    "            char_idx = 2\n",
    "        elif (char==\"T\" or char==\"t\"):\n",
    "            char_idx = 3\n",
    "        elif (char==\"N\" or char==\"n\"):\n",
    "            continue #leave that pos as all 0's\n",
    "        else:\n",
    "            raise RuntimeError(\"Unsupported character: \"+str(char))\n",
    "        if (one_hot_axis==0):\n",
    "            zeros_array[char_idx,i] = 1\n",
    "        elif (one_hot_axis==1):\n",
    "            zeros_array[i,char_idx] = 1\n",
    "\n",
    "onehot_data = np.array([one_hot_encode_along_row_axis(seq) for seq in data.sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "predictions = model.predict(onehot_data)\n",
    "\n",
    "print(roc_auc_score(y_score=predictions[:,0],\n",
    "                    y_true=data.labels[:,0]))\n",
    "print(roc_auc_score(y_score=predictions[:,1],\n",
    "                    y_true=data.labels[:,1]))\n",
    "print(roc_auc_score(y_score=predictions[:,2],\n",
    "                    y_true=data.labels[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the converted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"keras2_conv1d_record_5_model_PQzyq_modelWeights.h5\")\n",
    "open(\"keras2_conv1d_record_5_model_PQzyq_modelJson.json\",'w').write(model.to_json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
