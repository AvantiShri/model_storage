{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the saved Keras 1.2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/avantishrikumar/Research/model_storage/deeplift/genomics\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert from theano to tensorflow format.ipynb\r\n",
      "Convert keras1.2 conv2d to keras2 conv1d model.ipynb\r\n",
      "keras2_conv1d_record_5_model_PQzyq_modelJson.json\r\n",
      "keras2_conv1d_record_5_model_PQzyq_modelWeights.h5\r\n",
      "record_5_model_PQzyq_modelJson.json\r\n",
      "record_5_model_PQzyq_modelWeights.h5\r\n",
      "sequences.simdata.gz\r\n",
      "test.txt.gz\r\n",
      "test_mean_normalise_weights.ipynb\r\n",
      "tf_order_record_5_model_PQzyq_modelJson.json\r\n",
      "tf_order_record_5_model_PQzyq_modelWeights.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-06-10 20:01:07--  https://raw.githubusercontent.com/AvantiShri/model_storage/4143cfce7e61611d4c42984578e420cd7556c4b6/deeplift/genomics/tf_order_record_5_model_PQzyq_modelWeights.h5\n",
      "Resolving raw.githubusercontent.com... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 150792 (147K) [application/octet-stream]\n",
      "Saving to: 'tf_order_record_5_model_PQzyq_modelWeights.h5'\n",
      "\n",
      "100%[======================================>] 150,792     --.-K/s   in 0.09s   \n",
      "\n",
      "2018-06-10 20:01:08 (1.58 MB/s) - 'tf_order_record_5_model_PQzyq_modelWeights.h5' saved [150792/150792]\n",
      "\n",
      "--2018-06-10 20:01:08--  https://raw.githubusercontent.com/AvantiShri/model_storage/4143cfce7e61611d4c42984578e420cd7556c4b6/deeplift/genomics/tf_order_record_5_model_PQzyq_modelJson.json\n",
      "Resolving raw.githubusercontent.com... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2276 (2.2K) [text/plain]\n",
      "Saving to: 'tf_order_record_5_model_PQzyq_modelJson.json'\n",
      "\n",
      "100%[======================================>] 2,276       --.-K/s   in 0s      \n",
      "\n",
      "2018-06-10 20:01:08 (20.9 MB/s) - 'tf_order_record_5_model_PQzyq_modelJson.json' saved [2276/2276]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm tf_order_record_5_model_PQzyq_modelWeights.h5\n",
    "!rm tf_order_record_5_model_PQzyq_modelJson.json\n",
    "!wget https://raw.githubusercontent.com/AvantiShri/model_storage/4143cfce7e61611d4c42984578e420cd7556c4b6/deeplift/genomics/tf_order_record_5_model_PQzyq_modelWeights.h5\n",
    "!wget https://raw.githubusercontent.com/AvantiShri/model_storage/4143cfce7e61611d4c42984578e420cd7556c4b6/deeplift/genomics/tf_order_record_5_model_PQzyq_modelJson.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras version 1.2.0\n",
      "keras json {\n",
      "    \"class_name\": \"Sequential\", \n",
      "    \"keras_version\": \"1.2.1\", \n",
      "    \"config\": [\n",
      "        {\n",
      "            \"class_name\": \"Convolution2D\", \n",
      "            \"config\": {\n",
      "                \"W_constraint\": null, \n",
      "                \"b_constraint\": null, \n",
      "                \"name\": \"convolution2d_1\", \n",
      "                \"activity_regularizer\": null, \n",
      "                \"trainable\": true, \n",
      "                \"dim_ordering\": \"tf\", \n",
      "                \"nb_col\": 11, \n",
      "                \"subsample\": [\n",
      "                    1, \n",
      "                    1\n",
      "                ], \n",
      "                \"init\": \"glorot_uniform\", \n",
      "                \"bias\": true, \n",
      "                \"nb_filter\": 50, \n",
      "                \"activation\": \"linear\", \n",
      "                \"input_dtype\": \"float32\", \n",
      "                \"b_regularizer\": null, \n",
      "                \"W_regularizer\": null, \n",
      "                \"nb_row\": 4, \n",
      "                \"batch_input_shape\": [\n",
      "                    null, \n",
      "                    4, \n",
      "                    200, \n",
      "                    1\n",
      "                ], \n",
      "                \"border_mode\": \"valid\"\n",
      "            }\n",
      "        }, \n",
      "        {\n",
      "            \"class_name\": \"Activation\", \n",
      "            \"config\": {\n",
      "                \"activation\": \"relu\", \n",
      "                \"trainable\": true, \n",
      "                \"name\": \"activation_1\"\n",
      "            }\n",
      "        }, \n",
      "        {\n",
      "            \"class_name\": \"Convolution2D\", \n",
      "            \"config\": {\n",
      "                \"W_constraint\": null, \n",
      "                \"b_constraint\": null, \n",
      "                \"name\": \"convolution2d_2\", \n",
      "                \"activity_regularizer\": null, \n",
      "                \"trainable\": true, \n",
      "                \"dim_ordering\": \"tf\", \n",
      "                \"nb_col\": 11, \n",
      "                \"subsample\": [\n",
      "                    1, \n",
      "                    1\n",
      "                ], \n",
      "                \"init\": \"glorot_uniform\", \n",
      "                \"bias\": true, \n",
      "                \"nb_filter\": 50, \n",
      "                \"b_regularizer\": null, \n",
      "                \"W_regularizer\": null, \n",
      "                \"nb_row\": 1, \n",
      "                \"activation\": \"linear\", \n",
      "                \"border_mode\": \"valid\"\n",
      "            }\n",
      "        }, \n",
      "        {\n",
      "            \"class_name\": \"Activation\", \n",
      "            \"config\": {\n",
      "                \"activation\": \"relu\", \n",
      "                \"trainable\": true, \n",
      "                \"name\": \"activation_2\"\n",
      "            }\n",
      "        }, \n",
      "        {\n",
      "            \"class_name\": \"AveragePooling2D\", \n",
      "            \"config\": {\n",
      "                \"name\": \"averagepooling2d_1\", \n",
      "                \"trainable\": true, \n",
      "                \"dim_ordering\": \"tf\", \n",
      "                \"pool_size\": [\n",
      "                    1, \n",
      "                    180\n",
      "                ], \n",
      "                \"strides\": [\n",
      "                    1, \n",
      "                    1\n",
      "                ], \n",
      "                \"border_mode\": \"valid\"\n",
      "            }\n",
      "        }, \n",
      "        {\n",
      "            \"class_name\": \"Flatten\", \n",
      "            \"config\": {\n",
      "                \"trainable\": true, \n",
      "                \"name\": \"flatten_1\"\n",
      "            }\n",
      "        }, \n",
      "        {\n",
      "            \"class_name\": \"Dense\", \n",
      "            \"config\": {\n",
      "                \"W_constraint\": null, \n",
      "                \"b_constraint\": null, \n",
      "                \"name\": \"dense_1\", \n",
      "                \"activity_regularizer\": null, \n",
      "                \"trainable\": true, \n",
      "                \"init\": \"glorot_uniform\", \n",
      "                \"bias\": true, \n",
      "                \"input_dim\": 50, \n",
      "                \"b_regularizer\": null, \n",
      "                \"W_regularizer\": null, \n",
      "                \"activation\": \"linear\", \n",
      "                \"output_dim\": 50\n",
      "            }\n",
      "        }, \n",
      "        {\n",
      "            \"class_name\": \"Activation\", \n",
      "            \"config\": {\n",
      "                \"activation\": \"relu\", \n",
      "                \"trainable\": true, \n",
      "                \"name\": \"activation_3\"\n",
      "            }\n",
      "        }, \n",
      "        {\n",
      "            \"class_name\": \"Dropout\", \n",
      "            \"config\": {\n",
      "                \"p\": 0.5, \n",
      "                \"trainable\": true, \n",
      "                \"name\": \"dropout_1\"\n",
      "            }\n",
      "        }, \n",
      "        {\n",
      "            \"class_name\": \"Dense\", \n",
      "            \"config\": {\n",
      "                \"W_constraint\": null, \n",
      "                \"b_constraint\": null, \n",
      "                \"name\": \"dense_2\", \n",
      "                \"activity_regularizer\": null, \n",
      "                \"trainable\": true, \n",
      "                \"init\": \"glorot_uniform\", \n",
      "                \"bias\": true, \n",
      "                \"input_dim\": 50, \n",
      "                \"b_regularizer\": null, \n",
      "                \"W_regularizer\": null, \n",
      "                \"activation\": \"linear\", \n",
      "                \"output_dim\": 3\n",
      "            }\n",
      "        }, \n",
      "        {\n",
      "            \"class_name\": \"Activation\", \n",
      "            \"config\": {\n",
      "                \"activation\": \"sigmoid\", \n",
      "                \"trainable\": true, \n",
      "                \"name\": \"activation_4\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "} \n",
      "\n",
      "layer weight names: [u'convolution2d_1/convolution2d_1_W', u'convolution2d_1/convolution2d_1_b', u'convolution2d_2/convolution2d_2_W', u'convolution2d_2/convolution2d_2_b', u'dense_1/dense_1_W', u'dense_1/dense_1_b', u'dense_2/dense_2_W', u'dense_2/dense_2_b'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avantishrikumar/anaconda/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import json\n",
    "f = h5py.File(\"tf_order_record_5_model_PQzyq_modelWeights.h5\")\n",
    "print(\"keras version\", f.attrs['keras_version'])\n",
    "print(\"keras json\", json.dumps(json.loads(open(\"tf_order_record_5_model_PQzyq_modelJson.json\").read()),\n",
    "                               indent=4),\"\\n\")\n",
    "print(\"layer weight names:\", [layer_name+\"/\"+x\n",
    "                              for layer_name in f['model_weights'].keys()\n",
    "                              for x in f['model_weights'][layer_name].attrs['weight_names']],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "keras version"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /Users/avantishrikumar/anaconda/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2.1.6\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print (\"keras version\",keras.__version__)\n",
    "import numpy as np\n",
    "\n",
    "#create a keras 2 model with the same architecture\n",
    "#set the weights for each layer using the hdf5\n",
    "#weights file\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv1D(filters=50, kernel_size=11,\n",
    "                              strides=1,\n",
    "                              input_shape=(200,4)))\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.Conv1D(filters=50, kernel_size=11,\n",
    "                              strides=1))\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(units=50))\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(units=3))\n",
    "model.add(keras.layers.Activation(\"sigmoid\"))\n",
    "model.build()\n",
    "\n",
    "#load the weights into each layer\n",
    "\n",
    "#first convolution\n",
    "model.layers[0].set_weights(\n",
    "    [(np.array(f['model_weights']['convolution2d_1/convolution2d_1_W'])\n",
    "              .squeeze().transpose((1,0,2))),\n",
    "     np.array(f['model_weights']['convolution2d_1/convolution2d_1_b'])])\n",
    "#second convolution\n",
    "model.layers[2].set_weights(\n",
    "    [np.array(f['model_weights']['convolution2d_2/convolution2d_2_W']).squeeze(),\n",
    "     np.array(f['model_weights']['convolution2d_2/convolution2d_2_b'])])\n",
    "#first dense layer\n",
    "model.layers[5].set_weights(\n",
    "    [np.array(f['model_weights']['dense_1/dense_1_W']),\n",
    "     np.array(f['model_weights']['dense_1/dense_1_b'])])\n",
    "#second dense layer\n",
    "model.layers[8].set_weights(\n",
    "    [np.array(f['model_weights']['dense_2/dense_2_W']),\n",
    "     np.array(f['model_weights']['dense_2/dense_2_b'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-06-10 20:01:12--  https://raw.githubusercontent.com/AvantiShri/model_storage/db919b12f750e5844402153233249bb3d24e9e9a/deeplift/genomics/sequences.simdata.gz\n",
      "Resolving raw.githubusercontent.com... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 629502 (615K) [application/octet-stream]\n",
      "Saving to: 'sequences.simdata.gz'\n",
      "\n",
      "100%[======================================>] 629,502     2.87MB/s   in 0.2s   \n",
      "\n",
      "2018-06-10 20:01:13 (2.87 MB/s) - 'sequences.simdata.gz' saved [629502/629502]\n",
      "\n",
      "--2018-06-10 20:01:13--  https://raw.githubusercontent.com/AvantiShri/model_storage/9aadb769735c60eb90f7d3d896632ac749a1bdd2/deeplift/genomics/test.txt.gz\n",
      "Resolving raw.githubusercontent.com... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2287 (2.2K) [application/octet-stream]\n",
      "Saving to: 'test.txt.gz'\n",
      "\n",
      "100%[======================================>] 2,287       --.-K/s   in 0s      \n",
      "\n",
      "2018-06-10 20:01:13 (28.7 MB/s) - 'test.txt.gz' saved [2287/2287]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm sequences.simdata.gz\n",
    "!wget https://raw.githubusercontent.com/AvantiShri/model_storage/db919b12f750e5844402153233249bb3d24e9e9a/deeplift/genomics/sequences.simdata.gz\n",
    "!rm test.txt.gz\n",
    "!wget https://raw.githubusercontent.com/AvantiShri/model_storage/9aadb769735c60eb90f7d3d896632ac749a1bdd2/deeplift/genomics/test.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import simdna\n",
    "except ImportError:\n",
    "    print(\"installing simdna package\")\n",
    "    !pip install -e \"git://github.com/kundajelab/simdna.git@0.4.0#egg=simdna\"\n",
    "    print(\"\\n**********************************************************\")\n",
    "    print(\"RESTART THE JUPYTER KERNEL TO PICK UP ON THE INSTALLATION!!!\")\n",
    "    print(\"************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import simdna.synthetic as synthetic\n",
    "reload(synthetic)\n",
    "reload(synthetic.core)\n",
    "import gzip\n",
    "data_filename = \"sequences.simdata.gz\"\n",
    "#read in the data in the testing set\n",
    "test_ids_fh = gzip.open(\"test.txt.gz\",\"rb\")\n",
    "ids_to_load = [x.rstrip(\"\\n\") for x in test_ids_fh]\n",
    "data = synthetic.read_simdata_file(data_filename, ids_to_load=ids_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#this model was trained on data one-hot encoded as a 2d image,\n",
    "#with the row-axis being the axis for one-hot encoding.\n",
    "def one_hot_encode_along_row_axis(sequence):\n",
    "    #theano dim ordering, uses row axis for one-hot\n",
    "    to_return = np.zeros((len(sequence),4), dtype=np.int8)\n",
    "    seq_to_one_hot_fill_in_array(zeros_array=to_return,\n",
    "                                 sequence=sequence,\n",
    "                                 one_hot_axis=1)\n",
    "    return to_return\n",
    "\n",
    "def seq_to_one_hot_fill_in_array(zeros_array, sequence, one_hot_axis):\n",
    "    assert one_hot_axis==0 or one_hot_axis==1\n",
    "    if (one_hot_axis==0):\n",
    "        assert zeros_array.shape[1] == len(sequence)\n",
    "    elif (one_hot_axis==1):\n",
    "        assert zeros_array.shape[0] == len(sequence)\n",
    "    #zeros_array should be an array of dim 4xlen(sequence), filled with zeros.\n",
    "    #will mutate zeros_array\n",
    "    for (i,char) in enumerate(sequence):\n",
    "        if (char==\"A\" or char==\"a\"):\n",
    "            char_idx = 0\n",
    "        elif (char==\"C\" or char==\"c\"):\n",
    "            char_idx = 1\n",
    "        elif (char==\"G\" or char==\"g\"):\n",
    "            char_idx = 2\n",
    "        elif (char==\"T\" or char==\"t\"):\n",
    "            char_idx = 3\n",
    "        elif (char==\"N\" or char==\"n\"):\n",
    "            continue #leave that pos as all 0's\n",
    "        else:\n",
    "            raise RuntimeError(\"Unsupported character: \"+str(char))\n",
    "        if (one_hot_axis==0):\n",
    "            zeros_array[char_idx,i] = 1\n",
    "        elif (one_hot_axis==1):\n",
    "            zeros_array[i,char_idx] = 1\n",
    "\n",
    "onehot_data = np.array([one_hot_encode_along_row_axis(seq) for seq in data.sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9891813919942302\n",
      "0.9884463000576744\n",
      "0.9970644319550381\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "predictions = model.predict(onehot_data)\n",
    "\n",
    "print(roc_auc_score(y_score=predictions[:,0],\n",
    "                    y_true=data.labels[:,0]))\n",
    "print(roc_auc_score(y_score=predictions[:,1],\n",
    "                    y_true=data.labels[:,1]))\n",
    "print(roc_auc_score(y_score=predictions[:,2],\n",
    "                    y_true=data.labels[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the converted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"keras2_conv1d_record_5_model_PQzyq_modelWeights.h5\")\n",
    "open(\"keras2_conv1d_record_5_model_PQzyq_modelJson.json\",'w').write(\n",
    "    json.dumps(model.get_config())\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
